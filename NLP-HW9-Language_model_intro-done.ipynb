{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели.\n",
    "Можно использовать данные из семинара или любые другие (можно брать только часть текста, если считается слишком долго). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг \\<start>  \n",
    "    - можете использовать тот же подход с матрицей вероятностей, но по строкам хронить биграмы, а по колонкам униграммы \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)\n",
    "    - у вас будут словари с индексами биграммов и униграммов, не перепутайте их при переводе индекса в слово - словарь биграммов будет больше словаря униграммов и все индексы из униграммного словаря будут формально подходить для словаря биграммов (не будет ошибки при id2bigram[unigram_id]), но маппинг при этом будет совершенно неправильным "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb46e0-0c23-4faf-b827-6edc237d3346",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в файле lit.txt несколько рандомных русских романов\n",
    "# отсюда https://github.com/dachelnokova/russian-novels/tree/main/texts\n",
    "\n",
    "# сам файл тут https://github.com/dachelnokova/nlp-homeworks-2023/blob/main/lit.txt\n",
    "txt = open('lit.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15495b72-bb21-4119-b1e4-6e24a9765f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина 13247168\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина\", len(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8380eca8-a063-4761-bf81-5fdd3964fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    normalized_text = [word for word in normalized_text if not any(char.isdigit() or char in ['—', '»'] for char in word)]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7073c5b5-45bb-4527-b4bb-abbec87f21bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_txt = normalize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4479543b-046e-4c62-8f24-25aa7be281c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина корпуса в токенах - 2039153\n",
      "Уникальных токенов - 127476\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина корпуса в токенах -\", len(norm_txt))\n",
    "print(\"Уникальных токенов -\", len(set(norm_txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8451fb41-c4d3-4253-94db-053445660357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 95281),\n",
       " ('в', 46973),\n",
       " ('не', 45077),\n",
       " ('что', 35806),\n",
       " ('он', 32358),\n",
       " ('на', 28491),\n",
       " ('с', 24866),\n",
       " ('я', 23350),\n",
       " ('как', 18382),\n",
       " ('его', 16163)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посчитаем, сколько раз встречаются слова и выведем самые частотные\n",
    "from collections import Counter\n",
    "vocab_txt = Counter(norm_txt)\n",
    "vocab_txt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04522543-7745-4895-869c-c2bf78e13336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сколько раз встречается слово\n",
    "vocab_txt['россия']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7737e5f-8cde-47ad-82bf-c3dc5896fa2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.046725772906692144),\n",
       " ('в', 0.02303554465996421),\n",
       " ('не', 0.02210574684685259),\n",
       " ('что', 0.01755925131660057),\n",
       " ('он', 0.01586835318389547),\n",
       " ('на', 0.013971977580887751),\n",
       " ('с', 0.012194278702971283),\n",
       " ('я', 0.011450832772234354),\n",
       " ('как', 0.009014527110030488),\n",
       " ('его', 0.00792633019690038)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Для того, чтобы превратить абсолютные частоты в вероятности, разделим на общее число слов в каждом корпусе\n",
    "probas_txt = Counter({word:c/len(norm_txt) for word, c in vocab_txt.items()})\n",
    "probas_txt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff8faa2e-8fc7-40a9-a92d-52c5e2652a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35cd687a-f8db-45e8-8269-0b7dd3b541a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для создания n-грамм\n",
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfb0fcc5-d215-4c43-8426-9ad0cfb1d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавим теги начала и конца предложений\n",
    "# два тега старта, потому что вероятность первого слова\n",
    "    # будет рассчитываться как вероятность 3-грамма \"старт-старт-первое слово\" поделить на частоту биграммы \"старт-старт\"\n",
    "sentences = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f96f85bd-f0e7-4546-9329-3ab83bdac063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148864"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ca834f-6e75-47b8-807f-070567c62ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# отложим 50 первый предложений для перплексии\n",
    "sentences = sentences[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05265140-21ac-4367-890f-1de2db4eed47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148814"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5c339e4-0479-46b4-abad-2dff45460ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# вероятность слова C = вероятность триграммы ABC поделить на вероятность биграммы AB\n",
    "# поэтому добавляем еще счетчик триграмм\n",
    "\n",
    "unigrams = Counter()\n",
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "\n",
    "for sentence in sentences:\n",
    "    unigrams.update(sentence)\n",
    "    bigrams.update(ngrammer(sentence, n=2))\n",
    "    trigrams.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54f86eb-3fec-4877-a29f-64e99d42f261",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Создание матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f8231d2-b3da-4da6-b7cd-b0534eebf2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# В каждой ячееке будет лежать вероятность получить слово C, после биграммы AB. Биграмма AB в колонке, слово C – в строке\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9704982-3901-49be-963d-2b4c137958b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# Создание матрицы для триграмм, где строки - биграммы, а колонки - униграммы\n",
    "matrix_trigrams = lil_matrix((len(bigrams), len(unigrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e2de3b6-446f-42ea-9e8b-9a4ab65719d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2bigram = list(bigrams)\n",
    "bigram2id = {bigram: i for i, bigram in enumerate(id2bigram)}\n",
    "id2unigram = list(unigrams)\n",
    "unigram2id = {unigram: i for i, unigram in enumerate(id2unigram)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b835baa9-b4f3-4ea9-93e1-c751af22091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trigram in trigrams:\n",
    "    word1, word2, word3 = trigram.split()\n",
    "    bigram = ' '.join([word1, word2])\n",
    "    # на пересечение биграммы и слова ставим вероятность встретить слово после биграммы\n",
    "    matrix_trigrams[bigram2id[bigram], unigram2id[word3]] =  (trigrams[trigram]/\n",
    "                                                                      bigrams[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23805136-f1e0-42cd-994a-7c566b72ffff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 1.97965245e-02 2.24441249e-03 1.00796968e-04\n",
      "  4.20726545e-02]\n",
      " [0.00000000e+00 0.00000000e+00 3.69993211e-02 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 1.60513644e-03 0.00000000e+00 4.68699839e-01\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 2.91262136e-02 3.23624595e-03 0.00000000e+00\n",
      "  6.47249191e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(matrix_trigrams[:5, :5].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36427c47-464e-4a8a-9506-345cc10c9d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность слова 'деле' после биграммы 'на самом'      ' 0.1935483870967742\n",
      "Вероятность слова 'интересном' после биграммы 'на самом' 0.03225806451612903\n"
     ]
    }
   ],
   "source": [
    "print(\"Вероятность слова 'деле' после биграммы 'на самом'      '\",  trigrams['на самом деле']/bigrams['на самом'])\n",
    "print(\"Вероятность слова 'интересном' после биграммы 'на самом'\",  trigrams['на самом дне']/bigrams['на самом'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f60468c-4e5d-43ef-98be-9f5a153347f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1935483870967742"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_trigrams[bigram2id['на самом'], unigram2id[\"деле\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2014eb04-8161-4839-96db-0ccf7ca2bfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03225806451612903"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_trigrams[bigram2id['на самом'], unigram2id[\"дне\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92ee27-bb07-4ca9-b726-91ef53a3b366",
   "metadata": {},
   "source": [
    "### Генерация текста"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31135b4-8525-42c3-9583-1c9d6fb62774",
   "metadata": {},
   "source": [
    "Логика генерации:\n",
    "Начинаем с биграммы <start> <start>: ищем ее по строкам, переходим в одно из самых больших значений вероятности, смотрим на название его столбца. Допустим, это оказывается слово \"я\". Потом мы должны склеить последнее слово из биграммы и новый выбранный токен: получаем \"<start> я\". Потом ищем биграмму \"<start> я\" по строкам, переходим в одно из самых больших рандомных значений вероятности (chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])), смотрим на название его столбца. Допустим, это оказывается слово \"иду\". Потом мы должны склеить последнее слово из биграммы и новый выбранный токен: получаем \"я иду\". Потом ищем биграмму \"я иду\" я по строкам, переходим в одно из самых больших рандомных значений вероятности (chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])), смотрим на название его столбца. Допустим, это оказывается слово \"в\". Потом мы должны склеить последнее слово из биграммы и новый выбранный токен: получаем \"иду в\". И так далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35c92ec0-9525-4622-ab8e-57edf7ddaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(matrix, bigram2id, id2unigram, n=100, start = '<start> <start>', do_sample=True):\n",
    "    text = []  # Создаем пустой список для хранения сгенерированного текста\n",
    "    current_bigram = start  # Устанавливаем текущую биграмму, с которой начнется генерация текста\n",
    "    \n",
    "    for i in range(n):  # для генерации каждого из n слов\n",
    "        current_bigram_idx = bigram2id[current_bigram]  # получаем индекс текущей последней биграммы в матрице\n",
    "        # случайным образом выбираем следующее слово, учитывая вероятности\n",
    "        chosen_word_idx = np.random.choice(matrix.shape[1], p=matrix[current_bigram_idx].toarray()[0]) \n",
    "        chosen_word = id2unigram[chosen_word_idx]  # получаем слово по выбранному индексу\n",
    "        text.append(chosen_word)  # добавляем слово в текст\n",
    "        \n",
    "        # формируем новую биграмму для следующей итерации\n",
    "        current_bigram = current_bigram.split()[1] + ' ' + chosen_word\n",
    "        \n",
    "        if chosen_word == '<end>':  # Если выбранное слово - '<end>', значит, предложение закончилось\n",
    "            current_bigram = '<start> <start>'  # Начинаем новое предложение\n",
    "        \n",
    "    return ' '.join(text)  # Возвращаем сгенерированный текст как строку, объединив элементы списка через пробелы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40dca21f-b977-4047-82c4-8f4df390efc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "прежде всего начнут рыться виноватого искать \n",
      " там увидим \n",
      " и все язык ему показывал \n",
      " генеральша помолчала \n",
      " нет я здешняя я вся холодею когда об этом он виноват но ты будто … у него нет но зато себя осужу и сожгу тебя \n",
      " дело шло о прежних московских временах и московских знакомых николая но мужчин не замечаете … смех у дверей \n",
      " степан аркадьич посоветовал ехать за нею вошла девушка небольшого роста в длинном пальто и с удивлением смотрел на нее \n",
      " райский не отставал смердяков от души желаю да это было совершенно бесполезно \n",
      " он не находил\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(matrix_trigrams, bigram2id, id2unigram, n=100).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "730b0b3f-0fb0-4cb6-b3d9-d2e22287bbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "базаров как ты всё сделаешь отвечала долли обращаясь к дуняше \n",
      " а не вынужденного не для нас показание раненного вами старика григория васильева \n",
      " как из-за марьи генриховны вдруг поднялась вспутанная голова доктора \n",
      " балашев обедал в клубе желчно скучать равнодушно поспорить в холостом обществе стало для него своею непроницаемостью глаза и он дико смотрел на лица мундиры и сюртуки та же улыбка глаз как и муж перекрестился \n",
      " глядите все \n",
      " возьмите меня отсюда чтобы мне умереть \n",
      " что-то черное и страшное есть в погреб полезут на чердак в темноту скверное ощущение \n",
      " понимаете нужно знать я ничего не\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(matrix_trigrams, bigram2id, id2unigram, n=100).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b967336-0f9e-4648-b813-cf8fe40730d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "так так \n",
      " это большое строение \n",
      " я поехал к парикмахеру потом завтракать потом в гимназии он не замечая генерала заглядывал в кошелек а потому насыщай их ибо имеешь права судить comme c est le médecin intime de la divine providence сказал он с неприятным чувством чего-то недоделанного сел в тени ракиты ловили удочками рыбу \n",
      " он не мог бы отказаться \n",
      " о своей жене \n",
      " этим закончилось главное христианское богослужение \n",
      " я найду сам \n",
      " а вот и вся мысль игры пропадает \n",
      " в которой находилась наташа эта молитва сильно подействовала на меня взглянет ажно прожжет \n",
      " теоретически а\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(matrix_trigrams, bigram2id, id2unigram, n=100).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54fa4feb-6041-4775-8954-1963d8b257c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ведь жив он жив и к князю андрею что он все-таки шел \n",
      " продолжал спрашивать председатель \n",
      " левин слушал их и читает ее исподтишка двум-трем благоволящим к нему друг сердечный проговорил он не может всё уже и поворот в другую дверь \n",
      " в приемный покой вошли доктор с фельдшером это ее пианино здесь вообще много нисколько нисколько \n",
      " маленький ростом дохтуров сидел прямо против вейротера с старательным жестом сказала наташа \n",
      " вскрикнула опять настасья которой разговор этот доставлял по-видимому неизъяснимое блаженство \n",
      " нет об чем умном ну я хочу оставьте \n",
      " монах я lise \n",
      " спросила фенечка принимая в обе\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(matrix_trigrams, bigram2id, id2unigram, n=100).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a378f5d-bbfa-4289-a752-297e894a5b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "самолюбия сказал левин дрожащим голосом игнатий никифорович и встал \n",
      " вон они лежат у галерейки ждут \n",
      " ну так воздержись от кофе не пей \n",
      " человек ведь это произведение искусства \n",
      " ах это вы его фигуру \n",
      " о том что он в первый раз пьер вполне оценил наслажденье еды когда хотелось есть \n",
      " паром причалил к другому все более и более сомневаясь в правде и свободе для красоты слога \n",
      " но когда катя уговаривала меня заняться школой в деревне ничего не заметил впечатления какое может быть виновен перед начальством как и мы жизни теперь не доросли до них дотронуться \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(matrix_trigrams, bigram2id, id2unigram, n=100).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef9567-4465-4f34-819a-f506301141da",
   "metadata": {},
   "source": [
    "### Расчет перплексии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d454d9a-8113-4a11-81c7-97def68c1585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6160.945819408754"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perplexity(logp, N): # принимает на вход логарифм совместной вероятности (logp) и общее число слов (N)\n",
    "    return np.exp((-1/N) * logp)\n",
    "\n",
    "\n",
    "def compute_join_trigram_model(text, word_counts, bigram_counts, trigram_counts):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for ngram in ngrammer(['<start>', '<start>'] + tokens + ['<end>'], n=3): # Используем функцию ngrammer для создания триграмм\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigram = ' '.join([word1, word2])\n",
    "        if bigram in bigram_counts and ngram in trigram_counts:\n",
    "            prob += np.log(trigram_counts[ngram]/bigram_counts[bigram])\n",
    "        else:\n",
    "            prob += np.log(2e-5)  # Сглаживание для недостающих триграмм\n",
    "    \n",
    "    return prob, len(tokens)\n",
    "\n",
    "ps_trigram = []\n",
    "for sent in sent_tokenize(txt[:50]): # берем отложенные предложения (первые 50)\n",
    "    prob, N = compute_join_trigram_model(sent, unigrams, bigrams, trigrams)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps_trigram.append(perplexity(prob, N))\n",
    "\n",
    "np.mean(ps_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "180c461a-86b8-4d0e-bab4-1237e33d68fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# перплексия этой модели для двача сильно меньше, всего 28.73794075859534\n",
    "# первые 50 предложений тут видимо совсем не похожи на остальной текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85092eda-70b3-4532-aa8e-fed016bf1e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23aadce9-7d7c-4576-8e9e-3f3c89177fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.028182153336275"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'Офицер,  товарищ Тушина, был убит в начале  дела,  и в  продолжение  часа  из  сорока  человек  прислуги  выбыли семнадцать, но артиллеристы все так же были веселы и оживлены'\n",
    "log_prob, N = compute_join_trigram_model(phrase, unigrams, bigrams, trigrams)\n",
    "perplexity(log_prob, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733858c",
   "metadata": {},
   "source": [
    "Измените функцию generate_with_beam_search так, чтобы она работала с моделью, которая учитывает два предыдущих слова. \n",
    "Сравните получаемый результат с первым заданием. \n",
    "Также попробуйте начинать генерацию не с нуля (подавая \\<start> \\<start>), а с какого-то промпта. Но помните, что учитываться будут только два последних слова, так что не делайте длинные промпты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f886a37-96cd-47e4-84de-6b55c46683f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем класс чтобы хранить каждый из лучей\n",
    "class Beam:\n",
    "    def __init__(self, sequence: list, score: float):\n",
    "        self.sequence: list = sequence\n",
    "        self.score: float = score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d834d74-7a39-4a28-8141-2829382fcfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_beam_search(matrix, bigram2id, id2unigram, n=100, max_beams=5, start= \"<start> <start>\"):\n",
    "    # изначально у нас один луч с заданным началом (start по дефолту)\n",
    "    start_bigram = start.split()\n",
    "    initial_node = Beam(sequence=start_bigram, score=np.log1p(0))\n",
    "    beams = [initial_node]\n",
    "    \n",
    "    for i in range(n):\n",
    "        # делаем n шагов генерации\n",
    "        new_beams = []\n",
    "        # на каждом шаге продолжаем каждый из имеющихся лучей\n",
    "        for beam in beams:\n",
    "            #print(\"Current beam sequence:\", beam.sequence)\n",
    "            # лучи которые уже закончены не продолжаем (но и не удаляем)\n",
    "            if beam.sequence[-1] == '<end>':\n",
    "                new_beams.append(beam)\n",
    "                continue\n",
    "             # Вынимаем последнюю биграмму из последовательности и ищем ее индекс\n",
    "            last_bigram = \" \".join(beam.sequence[-2:])\n",
    "            last_bigram_id = bigram2id.get(last_bigram)\n",
    "\n",
    "            # посмотрим вероятности продолжений для последней биграммы в матрице\n",
    "            probas = matrix.getrow(last_bigram_id).toarray()[0]\n",
    "\n",
    "            # возьмем топ n самых вероятных продолжений\n",
    "            # top_idxs = probas.argsort()[:-(max_beams+1):-1]\n",
    "            top_idxs = np.random.choice(matrix.shape[1],\n",
    "                                        size=min(max_beams,probas.astype(bool).sum()),\n",
    "                                        p=probas, replace=False)\n",
    "                \n",
    "            for top_id in top_idxs: # итерируемся по индексам \n",
    "                # иногда вероятности будут нулевые, такое не добавляем\n",
    "                if not probas[top_id]:\n",
    "                    break\n",
    "                \n",
    "                # для каждого варианта продолжения создадим новую ветку, которая включает все предыдущее и новое слово\n",
    "                new_sequence = beam.sequence + [id2unigram[top_id]]\n",
    "                # создаем новый скор каждого луча это произведение вероятностей (или сумма логарифмов)\n",
    "                new_score = beam.score + np.log1p(probas[top_id])\n",
    "                new_beam = Beam(sequence=new_sequence, score=new_score)\n",
    "                new_beams.append(new_beam)\n",
    "        # отсортируем лучи по скору и возьмем только топ n max_beams\n",
    "        beams = sorted(new_beams, key=lambda x: x.score, reverse=True)[:max_beams]\n",
    "    \n",
    "    # в конце возвращаем самый вероятный луч\n",
    "    best_sequence = max(beams, key=lambda x: x.score).sequence\n",
    "\n",
    "    return ' '.join(best_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0093f032-be42-4758-bd04-b31e06bb467a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> <start> защитник же продолжал пользоваться всеми средствами этому нововведению но дело действительно разнеслось и достигло ушей начальства уши-то ведь у гоголя из писателей говорят эта черта совершенное отсутствие кокетства особенно удивляла и потому николушка заведя своего собственного таланта ни остановиться на чем-нибудь одном мешали плотно сложиться тоске о сыне она предложила наташе говеть и говеть не так живет не для нужд своих жить а живут войска курящие трубки раскладывающие костры на сенатской площади из сенатских стульев и варящие себе есть здесь бедный но милостыни еще до пробуждения ее являлись еще новые вопросы все пытались разрешать их писали читали говорили проекты всё\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_trigrams, bigram2id, id2unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "880e526f-b918-457a-bb0a-b2040c5fcac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я вас батюшка пригласил теперь по-домашнему совершенно этак ни с кем кроме соседа то есть прекращения отношений с благовоспитанными людьми своего привычного круга как будто отыскивая свой чепец через мельницу но другие поднятые воротники будут вас бить холодом пока вы живы продолжал он похаживая по комнате ходит да красные пятна зарделись на щеках смягчая смуглый цвет лица красиво подстриженные с сильной проседью волосы на висках и около губ ее улыбка обращенная к нему навстречу он заключил тоже что деньгу нажить любит наживает на злые проценты дает пройдоха шельма без жалости без содрогания сердца с энтузиазмом проговорил коля задрожавшим голоском побледнев и чувствуя\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_trigrams, bigram2id, id2unigram, start= \"я вас\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6b6db07-b25a-4d70-8ec6-661fc806e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# кажется, beam search работает получше, генерирует более связные предложения"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
