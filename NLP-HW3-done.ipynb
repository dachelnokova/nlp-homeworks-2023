{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c678e33-7efc-47da-bd84-890daf4b5beb",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "Имплементируйте алгоритм Леска (описание есть в семинаре) и оцените качество его работы на датасете `data/corpus_wsd_50k.txt`\n",
    "\n",
    "В качестве метрики близости вы должны попробовать два подхода:\n",
    "\n",
    "1) Jaccard score на множествах слов (определений и контекста)\n",
    "2) Cosine distance на эмбедингах sentence_transformers\n",
    "\n",
    "В качестве метрики используйте accuracy (% правильных ответов). Предсказывайте только многозначные слова в датасете\n",
    "\n",
    "Контекст вы можете определить самостоятельно (окно вокруг целевого слова или все предложение). Также можете поэкспериментировать с предобработкой для обоих методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be0ef724-f249-467f-be94-b8df53d73efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dariachelnokova/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79599516-c61d-454d-8ac0-893b83e48e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "153a4330-4140-4507-a743-18264a8ae784",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wsd = []\n",
    "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
    "for sent in corpus:\n",
    "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3487c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7f1976",
   "metadata": {},
   "source": [
    "## 1.1. Jaccard score на множествах слов (определений и контекста)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bf85646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля правильных ответов: 54.5%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "true = 0  # Количество правильных ответов\n",
    "total = 0  # Общее количество слов\n",
    "window_size = 4\n",
    "for sent in corpus_wsd[0:2000]:\n",
    "    for word_index, word_info in enumerate(sent):\n",
    "        # Извлекаем информацию о значении, лемме и слове по индексам\n",
    "        meaning = word_info[0]\n",
    "        lemma = word_info[1]\n",
    "        word = word_info[2]\n",
    "        \n",
    "        # Проверяем, является ли слово многозначным (имеет значение)\n",
    "        if not meaning:\n",
    "            continue  # Если слово однозначное, пропускаем его и переходим к следующему\n",
    "        \n",
    "        total += 1  # Увеличиваем счетчик общего числа оцененных многозначных слов\n",
    "        \n",
    "        # Создаем окно вокруг слова\n",
    "        start_index = max(0, word_index - window_size)\n",
    "        end_index = min(len(sent), word_index + window_size + 1)\n",
    "        context_words = [sent[i][2] for i in range(start_index, end_index) if i != word_index]\n",
    "        \n",
    "        # Удаляем стоп-слова из контекста\n",
    "        context_words = [word for word in context_words if word.lower() not in stop_words]\n",
    "        \n",
    "        # Создаем контекст из обработанных слов\n",
    "        context = \" \".join(context_words)\n",
    "        \n",
    "        # Создаем множество слов из контекста\n",
    "        context_set = set(context.split())\n",
    "        \n",
    "        best_meaning = None # Хранения наилучшего значения слова из WordNet\n",
    "        best_jaccard = float('inf')  # Хранения наивысшего Jaccard Score\n",
    "        \n",
    "        # Проходимся по всем синсетам слова из WordNet\n",
    "        for synset in wn.synsets(lemma):\n",
    "            # Получаем определение и создаем множество слов из определения\n",
    "            meaning_words = set(synset.definition().split())\n",
    "            \n",
    "            # Удаление стоп-слов из определения\n",
    "            meaning_words = [word for word in meaning_words if word.lower() not in stop_words]\n",
    "            \n",
    "            # Вычисляем Jaccard Score между контекстом и определением\n",
    "            intersection = len(context_set & set(meaning_words))\n",
    "            union = len(context_set | set(meaning_words))\n",
    "            jaccard_score = intersection / union\n",
    "            \n",
    "            # Если текущий Jaccard Score лучше (меньше) предыдущего лучшего\n",
    "            if jaccard_score < best_jaccard:\n",
    "                best_jaccard = jaccard_score\n",
    "                best_meaning = synset\n",
    "                \n",
    "        # Сравниваем наилучшее значение с фактическим значением многозначного слова из WordNet\n",
    "        if best_meaning == wn.lemma_from_key(meaning).synset():\n",
    "            true += 1  # Если значения совпали, увеличиваем счетчик правильных ответов\n",
    "\n",
    "# Вычисляем процент правильных ответов\n",
    "accuracy_percentage = (true / total) * 100\n",
    "rounded_accuracy = round(accuracy_percentage, 1)\n",
    "print(f'Доля правильных ответов: {rounded_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d0616b",
   "metadata": {},
   "source": [
    "## 1.2. Cosine distance на эмбедингах sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25c4a978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: torchaudio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (1.24.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.34.0)\n",
      "Requirement already satisfied: accelerate in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers) (0.16.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers) (1.24.2)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers) (1.3.1)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers) (0.17.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision->sentence_transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install torch torchvision torchaudio\n",
    "!python3 -m pip install sentence_transformers transformers accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7989b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b24a57bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dariachelnokova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент правильных ответов: 39.2%\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "true = 0  # Количество правильных ответов\n",
    "total = 0  # Общее количество слов\n",
    "window_size = 4\n",
    "\n",
    "def embed(text):\n",
    "    return model.encode(text, convert_to_tensor=True)\n",
    "\n",
    "for sent in corpus_wsd[0:20]:\n",
    "    for word_index, word_info in enumerate(sent):\n",
    "        # Извлекаем информацию о значении, лемме и слове по индексам\n",
    "        meaning = word_info[0]\n",
    "        lemma = word_info[1]\n",
    "        word = word_info[2]\n",
    "        \n",
    "        # Проверяем, является ли слово многозначным (имеет значение)\n",
    "        if not meaning:\n",
    "            continue  # Если слово однозначное, пропускаем его и переходим к следующему\n",
    "        \n",
    "        total += 1  # Увеличиваем счетчик общего числа оцененных многозначных слов\n",
    "        \n",
    "        # Создаем окно вокруг слова\n",
    "        start_index = max(0, word_index - window_size)\n",
    "        end_index = min(len(sent), word_index + window_size + 1)\n",
    "        context_words = [sent[i][2] for i in range(start_index, end_index) if i != word_index]\n",
    "        \n",
    "        # Удаляем стоп-слова из контекста\n",
    "        context_words = [word for word in context_words if word.lower() not in stop_words]\n",
    "        \n",
    "        # Создаем контекст из обработанных слов\n",
    "        context = \" \".join(context_words)\n",
    "        \n",
    "        # Получаем значения слова из WordNet\n",
    "        definitions = [synset.definition() for synset in wn.synsets(lemma)]\n",
    "        \n",
    "        # Вычисляем эмбеддинги для контекста и определений\n",
    "        context_embedding = embed(context)\n",
    "        definition_embeddings = [embed(definition) for definition in definitions]\n",
    "        \n",
    "        # Вычисляем Cosine Distance между контекстом и определениями\n",
    "        min_distance = float('inf')\n",
    "        best_meaning = None\n",
    "        \n",
    "        for i, definition_embedding in enumerate(definition_embeddings):\n",
    "            distance = cosine_distances([context_embedding.numpy()], [definition_embedding.numpy()])[0][0] \n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                best_meaning = wn.synsets(lemma)[i]\n",
    "                \n",
    "        # Сравниваем наилучшее значение с фактическим значением многозначного слова из WordNet\n",
    "        if best_meaning == wn.lemma_from_key(meaning).synset():\n",
    "            true += 1  # Если значения совпали, увеличиваем счетчик правильных ответов\n",
    "\n",
    "# Вычисляем процент правильных ответов и выводим результат\n",
    "accuracy_percentage = (true / total) * 100\n",
    "rounded_accuracy = round(accuracy_percentage, 1)\n",
    "print(f'Процент правильных ответов: {rounded_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efde9a-af0b-4c94-bfd0-249e7054562f",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "Попробуйте разные алгоритмы кластеризации на датасете - `https://github.com/nlpub/russe-wsi-kit/blob/initial/data/main/wiki-wiki/train.csv`\n",
    "\n",
    "Используйте код из семинара как основу. Используйте ARI как метрику качества.\n",
    "\n",
    "Попробуйте все 4 алгоритма кластеризации, про которые говорилось на семинаре. Для каждого из алгоритмов попробуйте настраивать гиперпараметры (посмотрите их в документации). Прогоните как минимум 5 экспериментов (не обязательно успешных) с разными параметрами на каждый алгоритме кластеризации и оцените: качество кластеризации, скорость работы, интуитивность параметров.\n",
    "\n",
    "Помимо этого также выберите 1 дополнительный алгоритм кластеризации отсюда - https://scikit-learn.org/stable/modules/clustering.html , опишите своими словами принцип его работы  и проделайте аналогичные эксперименты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59bef3e-1af7-4ce2-b43a-dfef282050f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/30/6f/910f62af8642c94acca4fff529944c1e9463cf118742f7ee1a583fc6449c/pandas-2.1.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading pandas-2.1.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.1.1-cp311-cp311-macosx_10_9_x86_64.whl (11.6 MB)\n",
      "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.1.1 pytz-2023.3.post1 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98e27031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "142a49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/nlpub/russe-wsi-kit/initial/data/main/wiki-wiki/train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49018a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_id</th>\n",
       "      <th>word</th>\n",
       "      <th>gold_sense_id</th>\n",
       "      <th>predict_sense_id</th>\n",
       "      <th>positions</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-5, 339-344</td>\n",
       "      <td>замок владимира мономаха в любече . многочисле...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-16, 17-22, 188-193</td>\n",
       "      <td>шильонский замок замок шильйон ( ) , известный...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>299-304</td>\n",
       "      <td>проведения архитектурно - археологических рабо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111-116</td>\n",
       "      <td>топи с . , л . белокуров легенда о завещании м...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134-139, 262-267</td>\n",
       "      <td>великий князь литовский гедимин после успешной...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>163-168, 213-218, 364-369</td>\n",
       "      <td>без каминов и очагов . у входа в башню было по...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>221-226, 276-281</td>\n",
       "      <td>v . в том сражении шотландцы потерпели сокруши...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-5, 16-21, 179-184</td>\n",
       "      <td>замок данноттар замок данноттар ( ) расположен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>232-237</td>\n",
       "      <td>. благодаря поэме байрона заключение бонивара ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5-10, 150-155, 289-294</td>\n",
       "      <td>года замок утратил свое военное значение , так...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    context_id   word  gold_sense_id  predict_sense_id  \\\n",
       "0            1  замок              1               NaN   \n",
       "1            2  замок              1               NaN   \n",
       "2            3  замок              1               NaN   \n",
       "3            4  замок              1               NaN   \n",
       "4            5  замок              1               NaN   \n",
       "..         ...    ...            ...               ...   \n",
       "95          96  замок              1               NaN   \n",
       "96          97  замок              1               NaN   \n",
       "97          98  замок              1               NaN   \n",
       "98          99  замок              1               NaN   \n",
       "99         100  замок              1               NaN   \n",
       "\n",
       "                    positions  \\\n",
       "0                0-5, 339-344   \n",
       "1       11-16, 17-22, 188-193   \n",
       "2                     299-304   \n",
       "3                     111-116   \n",
       "4            134-139, 262-267   \n",
       "..                        ...   \n",
       "95  163-168, 213-218, 364-369   \n",
       "96           221-226, 276-281   \n",
       "97        0-5, 16-21, 179-184   \n",
       "98                    232-237   \n",
       "99     5-10, 150-155, 289-294   \n",
       "\n",
       "                                              context  \n",
       "0   замок владимира мономаха в любече . многочисле...  \n",
       "1   шильонский замок замок шильйон ( ) , известный...  \n",
       "2   проведения архитектурно - археологических рабо...  \n",
       "3   топи с . , л . белокуров легенда о завещании м...  \n",
       "4   великий князь литовский гедимин после успешной...  \n",
       "..                                                ...  \n",
       "95  без каминов и очагов . у входа в башню было по...  \n",
       "96  v . в том сражении шотландцы потерпели сокруши...  \n",
       "97  замок данноттар замок данноттар ( ) расположен...  \n",
       "98  . благодаря поэме байрона заключение бонивара ...  \n",
       "99  года замок утратил свое военное значение , так...  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d235254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('word')[['word', 'context', 'gold_sense_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a438ed80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word                                            context  gold_sense_id\n",
      "383  бор  бор ( элемент ) бор — элемент тринадцатой груп...              1\n",
      "384  бор  бор - углерод - кремний семейство сплавов на о...              1\n",
      "385  бор  с большим выделением теплоты , образуется окси...              1\n",
      "386  бор  это объясняется прежде всего тем , что у компл...              1\n",
      "387  бор  совсем малочисленна . элементарный бор в приро...              1\n",
      "388  бор  действующем при месторождении горно - химическ...              1\n",
      "389  бор  b c ) . при нагревании в атмосфере кислорода и...              1\n",
      "390  бор  собственных минералов бора в чужих минералах о...              1\n",
      "391  бор  бор - углерод - кремний семейство сплавов на о...              1\n",
      "392  бор  с другими галогенами с образованием тригалоген...              1\n",
      "393  бор  и в стабильны и входят в состав природного бор...              1\n",
      "394  бор  номером . обозначается символом b ( ) . в своб...              1\n",
      "395  бор  и взаимные переходы которых определяются темпе...              1\n",
      "396  бор  . оксид бора formula_  — типичный кислотный ок...              1\n",
      "397  бор  бор ( растение ) бор ( ) — род растений семейс...              2\n",
      "398  бор  каштакский бор каштакский бор — бор , находящи...              2\n",
      "399  бор  бор ( растение ) бор ( ) — род растений семейс...              2\n",
      "400  бор  сосновый бор ( город ) сосно́вый бор — город (...              2\n",
      "401  бор  образом искусственный остров , поэтому для сое...              2\n",
      "402  бор   км к северу от города бузулук в обширной прир...              2\n",
      "403  бор  образовавшийся таким образом сросткинский сосн...              2\n",
      "404  бор  григоровский бор григоровский бор — историческ...              2\n",
      "405  бор  ичалковский бор площадью гектаров отнесен к ка...              2\n",
      "406  бор  его трассе ( на старых картах отчетливо видно ...              2\n",
      "407  бор  . в xix веке в серебряном бору размещался арти...              2\n",
      "408  бор  пески , мощность которых местами достигает мет...              2\n",
      "409  бор  заказника и его статус определяются положением...              2\n",
      "410  бор  каштакский бор каштакский бор — бор , находящи...              2\n",
      "411  бор  на территории бора действовал государственный ...              2\n",
      "412  бор  . фридлянда и инженера а . а . белоголового бы...              2\n",
      "413  бор  до $ . в настоящее время на территории серебря...              2\n",
      "414  бор  серебряный бор есть теория , что название сере...              2\n",
      "415  бор  бузулукский бор бузулу́кский бор — островной м...              2\n",
      "416  бор  . во второй половине xx века , сразу после вел...              2\n",
      "417  бор  , лента бора срастается с соседней лентой касм...              2\n",
      "418  бор  была существенно увеличена в дальнейшем на тер...              2\n",
      "419  бор  бузулук в обширной приречной котловине , имеющ...              2\n",
      "420  бор  ичалковский бор специальным распоряжением сове...              2\n",
      "421  бор  к металлургическому району города . древостой ...              2\n",
      "422  бор  , которые понравятся и детям , и взрослым . в ...              2\n",
      "423  бор  колонии цапли , а в лесу гнездились соколы — п...              2\n",
      "424  бор  серебряном бору , в отдельный подвид сосны обы...              2\n",
      "425  бор  окрестностях семипалатинска . ширина бора на з...              2\n",
      "426  бор  года №  ичалковский бор , его пещеры и карсты ...              2\n",
      "427  бор  серебряный бор есть теория , что название сере...              2\n",
      "428  бор  нашлось необходимого обоснования ( по краеведч...              2\n",
      "429  бор  бор ( лес ) различают следующие основные групп...              2\n",
      "430  бор  ичалковский бор специальным распоряжением сове...              2\n",
      "431  бор  , км² . от самого села до бора можно дойти по ...              2\n",
      "432  бор  озеро на дне холодной пещеры даже в теплое вре...              2\n",
      "433  бор  серебряный бор есть теория , что название сере...              2\n",
      "434  бор  ленточный бор ле́нточные бо́ры — сосновые трав...              2\n",
      "435  бор  в окрестностях барнаула , составляет —  км . н...              2\n",
      "436  бор  также в сосновом бору открыта секция биатлона ...              2\n",
      "437  бор  экспресс банк , мособлбанк , внешпромбанк , ба...              2\n",
      "438  бор  каштакский бор каштакский бор — бор , находящи...              2 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key, _ in grouped_df:\n",
    "    print(grouped_df.get_group(key), \"\\n\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8b69547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AffinityPropagation, DBSCAN, AgglomerativeClustering, MeanShift\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41fe910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embed = model.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "987dfec2-76ca-41c5-a21c-b94df9c589df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группируем данные по словам\n",
    "grouped_df = df.groupby('word')[['word', 'context', 'gold_sense_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "688086f8-40d1-4b7b-8282-091c3a3bbc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем список для хранения ARI для каждого слова\n",
    "ARI = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a6dc417-f4dd-47db-a93c-a5856aec71ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, _ in grouped_df:\n",
    "    # Вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "212ba622-37f3-4a40-8eb7-26963f9f15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Создаем пустую матрицу для векторов\n",
    "    X = np.zeros((len(texts), 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8912be6d-fc90-4828-86bb-e63df3347043",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "725bc880-9b7f-495d-a949-4ada4c45808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # выбираем один из алгоритмов\n",
    "    cluster = KMeans(n_clusters=3, init='k-means++')\n",
    "#    cluster = KMeans(n_clusters=10, init='random')\n",
    "#    cluster = KMeans(n_clusters=3, init=10)\n",
    "#    cluster = KMeans(n_clusters=5, n_init=20)\n",
    "#    cluster = KMeans(n_clusters=10, n_init=5)\n",
    "    \n",
    "#    cluster = AffinityPropagation(damping=0.5)\n",
    "#    cluster = AffinityPropagation(damping=0.6, preference=5)\n",
    "#    cluster = AffinityPropagation(damping=0.7, preference=-3, affinity ='precomputed')\n",
    "#    cluster = AffinityPropagation(damping=0.8, preference=20, convergence_iter =\"100\")\n",
    "#    cluster = AffinityPropagation(damping=0.9, preference=10)\n",
    "\n",
    "\n",
    "#    cluster = DBSCAN(eps=0.5, min_samples=5, metric='euclidean', algorithm='auto', leaf_size=30)\n",
    "#    cluster = DBSCAN(eps=0.2, min_samples=10, metric='manhattan', algorithm='ball_tree', leaf_size=20)\n",
    "#    cluster = DBSCAN(eps=0.3, min_samples=3, metric='cosine', algorithm='kd_tree', leaf_size=40)\n",
    "#    cluster = DBSCAN(eps=0.4, min_samples=8, metric='chebyshev', algorithm='auto', leaf_size=50)\n",
    "#    cluster = DBSCAN(eps=0.6, min_samples=12, metric='mahalanobis', algorithm='auto', leaf_size=30)\n",
    "\n",
    "\n",
    "#    cluster = AgglomerativeClustering(n_clusters=3)\n",
    "#    cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "#    cluster = AgglomerativeClustering(n_clusters=6, affinity='manhattan', linkage='complete')\n",
    "#    cluster = AgglomerativeClustering(n_clusters=3, affinity='cosine', linkage='average')\n",
    "#    cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129b0c3",
   "metadata": {},
   "source": [
    "### Mean Shift\n",
    "\n",
    "Принцип работы Mean Shift заключается в смещении \"средней точки\"  в пространстве данных таким образом, чтобы она двигалась к месту, где плотность точек больше,  пока не достигнет максимума плотности.  Каждая точка будет отнесена к одному из кластеров на основе того, к какой средней точке она ближе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6061dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#    cluster = MeanShift(bandwidth=0.5)\n",
    "#    cluster = MeanShift(bin_seeding=True, min_bin_freq=5)\n",
    "#    cluster = Mean_Shift(max_iter=100, cluster_all=False)\n",
    "#    cluster = MeanShift(max_iter=200, bandwidth=0.6)\n",
    "#    cluster = MeanShift(bin_seeding=True, min_bin_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e463b3f0-ba35-4d64-891b-ba63e10e5a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.015141540186292346\n"
     ]
    }
   ],
   "source": [
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be1d53-f7fa-44a5-9056-6cb6e189cb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fef5b5-2545-4544-8177-e3f75dab8757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
